{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW# get_linear_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from Custom_Dataset_Class import *\n",
    "from BertFineTuned import *\n",
    "from LongTerm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro T2000\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Nettoyage des données\n",
      "\n",
      "=============== EPOCH 1 / 1 ===============\n",
      "\n",
      "___ batch index = 0 / 201 (0.00%), loss = 2.2960, time = 4.72 secondes ___\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9f11c8198fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mbatches_losses_tmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loop_fun1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_losses_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bureau/ppd/utils.py\u001b[0m in \u001b[0;36mtrain_loop_fun1\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE=4\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    overlap_len=OVERLAP_LEN,\n",
    "    max_size_dataset=MAX_SIZE_DATASET)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=BertConsumerComplaints().to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "t0 = time.time()\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    print(f\"=====>\\t{evaluate(target.reshape(-1), output)}\")\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"model1/model_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN classifier on top of the Fine Tuned Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=4\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    overlap_len=OVERLAP_LEN,\n",
    "    max_size_dataset=MAX_SIZE_DATASET)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model_rnn=LongTermModel(bertFineTuned=list(model.children())[0]).to(device)\n",
    "optimizer=AdamW(model_rnn.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "t0 = time.time()\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_rnn, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_rnn, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    print(f\"=====>\\t{evaluate(target.reshape(-1), output)}\")\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ the RNN model has been saved §§\")\n",
    "    torch.save(model_rnn, f\"model_rnn1/model_rnn_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
